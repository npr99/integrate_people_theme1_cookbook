{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmpnvPt2LEPX"
   },
   "source": [
    "# Read in Hourly Air Data\n",
    "Yo Kimura generated files that have hourly air measures. Files are very large. Need to see if it is possible to read them in and clean them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help from MS Copilot LLM\n",
    "Annual Inhaled Mass (AIM): How much of the chemical a person would inhale if they stayed in that cell all year. (µg)\n",
    "\n",
    "Annual Absorbed Dose Index (ADI): How much is taken up into blood (not just inhaled). (µg)\n",
    "\n",
    "Also may make sense to just sum across all of the values for each grid cell - or each hour for each grid cell\n",
    "\n",
    "Question - if we know pressure and temperature and the age of the person we could be more detailed in AIM and ADI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkA2Z92rLEPd"
   },
   "source": [
    "## Description of Program\n",
    "- program:    ip1_2cv1_HourlyAir\n",
    "- task:       Read Air files with hourly data\n",
    "- Version:    2026-01-09\n",
    "- v2:         Consolidate code and prepare to loop\n",
    "- v3:         Explore options for aggregated annual values\n",
    "- project:    Southeast Texas Urban Integrated Field Lab\n",
    "- funding:\t  DOE\n",
    "- author:     Nathanael Rosenheim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzVM0iMjLEPe"
   },
   "source": [
    "## Step 0: Good Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJZ1ezndLEPf"
   },
   "outputs": [],
   "source": [
    "# 1. Import all packages\n",
    "import pandas as pd     # For obtaining and cleaning tabular data\n",
    "import os # For saving output to path\n",
    "import zipfile # For handling zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtyVl_YjLEPg"
   },
   "outputs": [],
   "source": [
    "# 2. Check versions\n",
    "import sys\n",
    "print(\"Python Version     \", sys.version)\n",
    "print(\"geopandas version: \", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check working directory\n",
    "# Get information on current working directory (getcwd)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ftzkt-jSLEPi"
   },
   "outputs": [],
   "source": [
    "#4. Store Program Name for output files to have the same name\n",
    "programname = \"ip1_2cv3_hourlyair\"\n",
    "# Make directory to save output\n",
    "if not os.path.exists(programname):\n",
    "    os.mkdir(programname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWMQBAqbLEPi"
   },
   "source": [
    "# Step 1: Obtain Data\n",
    "Obtain CSV Files\n",
    "\n",
    "Posted CSV dump of the camx model ouput.\n",
    "https://utexas.app.box.com/folder/359619230313\n",
    "\n",
    "Nathanael saved an example file (a small one) on his local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdM1zI9CLEPj"
   },
   "source": [
    "# Step 2: Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_hourly_air_quality_data(folder_name, pollutant_name=\"benz\", resolution=\"1km\"):\n",
    "    # read in csv file from SourceData\\Kimura_Hourly_2026-01-08\\hourly_benz_1km.zip\n",
    "    zip_path = os.path.join(\"SourceData\", folder_name, f\"hourly_{pollutant_name}_{resolution}.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        with z.open(f'hourly_{pollutant_name}_{resolution}.csv') as f:\n",
    "            hourly_df = pd.read_csv(f)\n",
    "\n",
    "    return hourly_df\n",
    "\n",
    "hourly_benz_df = obtain_hourly_air_quality_data(\"Kimura_Hourly_2026-01-08\", pollutant_name=\"benz\", resolution=\"1km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_benz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust \n",
    "hourly_benz_df[['TSTEP','BENZ']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many hours are in the data?\n",
    "hourly_benz_df['tstamp'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5136/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ROW and COL and sum the BENZ values\n",
    "summed_benz_df = hourly_benz_df.groupby(['ROW', 'COL'])['BENZ'].sum().reset_index()\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "summed_benz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique ROW, COL, x, and y combinations from the original dataframe\n",
    "coords_df = hourly_benz_df[['ROW', 'COL', 'x', 'y']].drop_duplicates()\n",
    "\n",
    "# Merge the coordinates back into the summed dataframe\n",
    "summed_benz_with_coords_df = pd.merge(summed_benz_df, coords_df, on=['ROW', 'COL'])\n",
    "\n",
    "# convert ROW and COL to integer\n",
    "summed_benz_with_coords_df['ROW'] = summed_benz_with_coords_df['ROW'].astype(int)\n",
    "summed_benz_with_coords_df['COL'] = summed_benz_with_coords_df['COL'].astype(int)\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "summed_benz_with_coords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats for BENZ\n",
    "summed_benz_with_coords_df['BENZ'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unique id based off ROW and COL\n",
    "def generate_grid_id(row, col, resolution=\"1km\"):\n",
    "    return f\"air{resolution}_{int(row):04d}_{int(col):04d}\"\n",
    "\n",
    "summed_benz_with_coords_df['grid_id'] = summed_benz_with_coords_df.apply(lambda row: \n",
    "                            generate_grid_id(row['ROW'], row['COL'], resolution=\"1km\"), \n",
    "                            axis=1)\n",
    "summed_benz_with_coords_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(summed_benz_with_coords_df['x'], summed_benz_with_coords_df['y'], c=summed_benz_with_coords_df['BENZ'], cmap='viridis', s=10)\n",
    "plt.colorbar(label='Sum of BENZ')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sum of BENZ by Location')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "summed_benz_with_coords_df.to_csv(os.path.join(programname, f\"{programname}_summed_benz_1km.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LISA Analysis Summary**\n",
    "\n",
    "LISA (Local Indicators of Spatial Association) identifies spatial clusters:\n",
    "- **Hot spots (High-High)**: High values surrounded by high neighbors\n",
    "- **Cold spots (Low-Low)**: Low values surrounded by low neighbors  \n",
    "- **Outliers**: High-Low or Low-High combinations\n",
    "\n",
    "**Why LISA for benzene grid data?**\n",
    "- Grid structure is ideal for neighbor definitions\n",
    "- Provides statistical significance testing\n",
    "- Standard method in environmental epidemiology\n",
    "\n",
    "**Implementation**: Use PySAL library (`esda.Moran_Local()` with `libpysal.weights.lat2W()` for grid weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spatial analysis libraries\n",
    "import geopandas as gpd\n",
    "from libpysal.weights import lat2W\n",
    "from esda.moran import Moran, Moran_Local\n",
    "import numpy as np\n",
    "\n",
    "# Import custom LISA analysis functions\n",
    "from ip1_3cv1_LISA_analysis import (\n",
    "    create_spatial_weights,\n",
    "    create_spatial_weights_knn,\n",
    "    calculate_global_morans_i,\n",
    "    plot_morans_i_scatterplot,\n",
    "    calculate_local_morans_i,\n",
    "    plot_lisa_cluster_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, df_sorted = create_spatial_weights(summed_benz_with_coords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_global = calculate_global_morans_i(df_sorted, 'BENZ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_morans_i_scatterplot(df_sorted, 'BENZ', w, moran_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted, moran_local = calculate_local_morans_i(df_sorted, 'BENZ', w, significance_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lisa_cluster_map(df_sorted, 'BENZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without cold spots from round 1 (remove ocean to see what happens)\n",
    "df_sorted_no_ocean = df_sorted[~df_sorted['cluster_label'].isin(['LL (Cold spot)'])].copy()\n",
    "# Use KNN weights instead of grid-based weights since we filtered cells\n",
    "w_knn, df_sorted_no_ocean = create_spatial_weights_knn(df_sorted_no_ocean, x_col='x', y_col='y', k=8)\n",
    "df_sorted_no_ocean, moran_local_no_ocean = calculate_local_morans_i(df_sorted_no_ocean, 'BENZ', w_knn, significance_level=0.05)\n",
    "plot_lisa_cluster_map(df_sorted_no_ocean, 'BENZ')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "HRRC_00_templateDSnotebook_2021-01-24.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ip1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
